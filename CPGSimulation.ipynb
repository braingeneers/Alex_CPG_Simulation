{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right\"><font size=7 color=\"orchid\" face='Brush Script MT'> to Start, Click - <button class=\"btn btn-sm btn-success\"><i class=\"fa fa-lg fa-id-card-o\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\" size=7><i>CPG Simulation</i></font>\n",
    "    \n",
    "<font color=\"gray\"><i>A small purpose built neural network uses proprioceptive feedback to control a modular robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "**<div class=\"text-center\">Abstract</div>**\n",
    "We propose a modular architecture for neuromorphic closed-loop control based on bistable relaxation oscillator modules consisting of three spiking neurons each. Like its biological prototypes, this basic component is robust to parameter variation but can be modulated by external inputs. By combining these modules, we can construct a neural state machine capable of generating the cyclic or repetitive behaviors necessary for legged locomotion. A concrete case study for the approach is provided by a modular robot constructed from flexible plastic volumetric pixels, in which we produce a forward crawling gait entrained to the natural frequency of the robot by a minimal system of twelve neurons organized into four modules.\n",
    "\n",
    "\n",
    "<div class=\"text-center\"><a href=\"https://doi.org/10.1371/journal.pone.0240267\">Paper Link</a></div>\n",
    "    \n",
    "\n",
    "    \n",
    "**Authors** Alex Spaeth, Maryam Tebyani, David Haussler, Mircea Teodorescu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML,Image\n",
    "import ipywidgets as ipw   #HTML(\"\"\"<h2 class='text-center text-danger'>Tutorial coming Soon</h2><div class=\"row\"><div class=\"col-xs-12 col-md-offset-3 col-md-6\"><div class=\"embed-responsive embed-responsive-16by9\"><iframe class=\"embed-responsive-item\"src='https://www.youtube.com/embed/pwBs4J4xDOw'></iframe></div></div></div>\"\"\")\n",
    "HTML(\"\"\"<div class=\"row\"><div class=\"col-xs-12\"><div class=\"embed-responsive embed-responsive-16by9\"><iframe class=\"embed-responsive-item\"src='https://www.youtube.com/embed/3h20uhEVuB0'></iframe></div></div></div>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neuron Simulation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section contains the parameters and code for simulation of the  neuron model. The dictionaries RS and LTS describe single-neuron parameters, and then there are a bunch of different methods producing parameter arrays suitable for the simulation code.\n",
    "\n",
    "The simulation code is in the function `solve_neurons`, which can perform a simulation until a fixed termination time or until steady state. It requires a parameter array µ, a synaptic connectivity matrix G, and an initial condition x0. The docstring describes the format of these parameters in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using MultivariateStats\n",
    "using RecursiveArrayTools\n",
    "\n",
    "import DifferentialEquations\n",
    "DE = DifferentialEquations\n",
    "\n",
    "import PyPlot\n",
    "Plt = PyPlot.plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\"Parameters for regular-spiking (RS) neuron.\"\n",
    "const RS = Dict(:a => 0.03, :b => -2.0, :c => -50.0, :d => 100.0, :C => 100.0, \n",
    "    :k => 0.7, :Vr => -60.0, :Vt => -40.0, :Vn => 0.0, :Vp => 35.0, :τ => 5.0)\n",
    "\"Parameters for low-threshold-spiking (LTS) interneuron.\"\n",
    "const LTS = Dict(:a => 0.03, :b => 8.0, :c => -53.0, :d => 20.0, :C => 100.0,\n",
    "    :k => 1.0, :Vr => -56.0, :Vt => -42.0, :Vn => -70.0, :Vp => 20.0, :τ => 20.0)\n",
    "\n",
    "\"Excitatory connection between two neurons of a single module.\"\n",
    "const G_exc = 20.0\n",
    "\"Excitatory connection keeping the reset input easy to activate.\"\n",
    "const G_rst = 5.0\n",
    "\"Inhibitory connection from the reset input of the module.\"\n",
    "const G_inh = 10.0\n",
    "\"Feedforward connection for gradual activation of other modules.\"\n",
    "const G_ffw = 10.0\n",
    "\"Feedback connection for deactivation of other modules.\"\n",
    "const G_fb = 10.0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Gij(; G_exc, G_inh, G_rst)\n",
    "\n",
    "Constructs a connectivity matrix for a single module given the three \n",
    "defining conductances.\n",
    "\"\"\"\n",
    "Gij(; G_exc=G_exc, G_inh=G_inh, G_rst=G_rst) = [0.0  G_exc G_inh;\n",
    "                                                G_exc  0.0  G_inh;\n",
    "                                                G_rst G_rst  0.0 ]\n",
    "\n",
    "\"\"\"\n",
    "    x0_fire1(μ)\n",
    "\n",
    "Given a parameter array μ, returns an initial condition just before a\n",
    "firing of the first cell.\n",
    "\"\"\"\n",
    "function x0_fire1(μ=μ_module)\n",
    "  Vr = μ.x[7]\n",
    "  Vp = μ.x[10]\n",
    "  d = μ.x[4]\n",
    "  ArrayPartition([Vp[1]; Vr[2:end]],\n",
    "                 zero(d),\n",
    "                 zeros(length(d)),\n",
    "                 zeros(length(d)))\n",
    "end\n",
    "\n",
    "\n",
    "\"All the names of the neuron parameters.\"\n",
    "parameter_names = (:a, :b, :c, :d, :C, :k, :Vr, :Vt, :Vn, :Vp, :τ)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_from_types(types)\n",
    "\n",
    "Assemble a parameter array from a list of neuron types.\n",
    "\"\"\"\n",
    "μ_from_types(types...) = ArrayPartition([[t[param] for t in types]\n",
    "                                         for param in parameter_names]...)\n",
    "\n",
    "\"Default parameter array for a module: two RS neurons and one LTS.\"\n",
    "μ_module = μ_from_types(RS, RS, LTS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia \n",
    "\"\"\"\n",
    "    jump_neuron!(i, x, μ)\n",
    "\n",
    "Given a state, perform the reset for the ith neuron.\n",
    "\"\"\"\n",
    "function jump_neuron!(i, x, μ)\n",
    "  x.x[1][i] = μ.x[3][i]\n",
    "  x.x[2][i] += μ.x[4][i]\n",
    "  x.x[4][i] += 1.0\n",
    "  x\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    jump_map(μ, x)\n",
    "\n",
    "Given a state, return a copy with all applicable spike resets applied.\n",
    "\"\"\"\n",
    "function jump_map(μ, x)\n",
    "  x = copy(x)\n",
    "  for (i,dv) in enumerate(x.x[1] .- μ.x[10])\n",
    "    if dv >= 0\n",
    "      jump_neuron!(i, x, μ)\n",
    "    end\n",
    "  end\n",
    "\n",
    "  x\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    solve_neurons(μ, G, x0)\n",
    "\n",
    "Integrate the forward dynamics of a system of N neurons. If a finite\n",
    "stop time tmax is provided, integrates until that point. Otherwise,\n",
    "integrates until the first spike of a cell listed in termination_indices,\n",
    "or until the system reaches a steady state.\n",
    "\n",
    "The neuron parameters are stored in an ArrayPartition μ with one\n",
    "component per parameter, containing N parameter values. The synaptic\n",
    "connectivity matrix G is just an N×N array. The initial condition x0\n",
    "should be an ArrayPartition with four components corresponding to the\n",
    "per-neuron initial values of v, u, x, and y.\n",
    "\"\"\"\n",
    "function solve_neurons(μ, G, x0; tmax=Inf, fp_tol=1e-4,\n",
    "                       termination_indices=[], solver_args...)\n",
    "\n",
    "  # The in-place continuous dynamics of the neurons.\n",
    "  function izh(dx, x, (μ,G), t)\n",
    "    a,b,c,d, C,k,vr,vt,vn,vp, τ = μ.x\n",
    "\n",
    "    sodium = @.k*(x.x[1] - vr)*(x.x[1] - vt)\n",
    "    Isyn = G*(x.x[3] .* vn)  .-  (G*x.x[3]) .* x.x[1]\n",
    "    dx.x[1] .= @. (sodium - x.x[2] + Isyn)/C\n",
    "    dx.x[2] .= @. a*(b*(x.x[1]-vr) - x.x[2])\n",
    "    dx.x[3] .= @. x.x[4] / τ\n",
    "    dx.x[4] .= @. -(x.x[3] + 2x.x[4]) / τ\n",
    "  end\n",
    "\n",
    "\n",
    "  # This is the callback that produces the spiking behavior. It is\n",
    "  # vectorized and done in-place for efficiency. The first function\n",
    "  # condition() checks whether any voltage exceeds the corresponding\n",
    "  # cell's Vp, and the second function affect!() does the reset for\n",
    "  # that cell (or terminates if it's on the termination list).\n",
    "  function condition(out, u,t,integrator)\n",
    "    out .= u.x[1] .- integrator.p[1].x[10]\n",
    "  end\n",
    "\n",
    "  function affect!(integrator, i)\n",
    "    if i in termination_indices\n",
    "      integrator.u.x[1][i] = integrator.p[1].x[10][i]\n",
    "      DE.terminate!(integrator)\n",
    "    else\n",
    "      jump_neuron!(i, integrator.u, integrator.p[1])\n",
    "    end\n",
    "  end\n",
    "\n",
    "  # The VectorContinuousCallback needs to know how many callbacks\n",
    "  # there are, which is the number of neurons. Get that information\n",
    "  # from the number of voltage values in the initial condition.\n",
    "  spike = DE.VectorContinuousCallback(condition, affect!, length(x0.x[1]))\n",
    "\n",
    "  # Handle infinite tmax by waiting for steady state. This code is\n",
    "  # taken from an example in the DifferentialEquations.jl docs and\n",
    "  # modified to set the time to infinity before terminating so it's\n",
    "  # clear that we've reached a steady-state solution.\n",
    "  callbacks = if tmax != Inf\n",
    "    spike\n",
    "  else\n",
    "    function allDerivPass(integrator, abstol, reltol)\n",
    "      if DE.isinplace(integrator.sol.prob)\n",
    "        testval = first(DE.get_tmp_cache(integrator))\n",
    "        DE.get_du!(testval, integrator)\n",
    "      else\n",
    "        testval = get_du(integrator)\n",
    "      end\n",
    "\n",
    "      !any(abs(d) > abstol && abs(d) > reltol*abs(u)\n",
    "           for (d,abstol, reltol, u) =\n",
    "           zip(testval, Iterators.cycle(abstol),\n",
    "               Iterators.cycle(reltol), integrator.u))\n",
    "    end\n",
    "\n",
    "    at_steady_state = (u, t, integrator) -> allDerivPass(integrator,\n",
    "                                                         fp_tol, fp_tol)\n",
    "    function terminate_steady_state!(integrator)\n",
    "      DE.set_t!(integrator, Inf)\n",
    "      DE.terminate!(integrator)\n",
    "    end\n",
    "    tss = DE.DiscreteCallback(at_steady_state,\n",
    "                              terminate_steady_state!;\n",
    "                              save_positions = (true, false))\n",
    "    DE.CallbackSet(spike, tss)\n",
    "  end\n",
    "\n",
    "  # Solve the problem and return it!\n",
    "  prob = DE.ODEProblem(izh, x0, (0.0, tmax), (μ,G))\n",
    "  DE.solve(prob, DE.Tsit5(), callback=callbacks; solver_args...)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mathematical Analysis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section of the notebook defines the functions that are used for the mathematical analysis of the neuron simulations that can be run using the above code. In particular, we define the Poincaré map of a neural system along the Poincaré section $V_1 = V_p$ as well as functions which find the fixed point and the corresponding return time, and all the various methods for random variations of neuron types that will be required for the Monte Carlo analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\"\"\"\n",
    "    poincaré(μ, G, x0)\n",
    "\n",
    "For an initial condition on the Poincaré section, perform a jump\n",
    "and compute the first return time and location in phase space.\n",
    "Returns infinite time if we never return to the Poincaré section\n",
    "due to finding a fixed point in the dynamics.\n",
    "\"\"\"\n",
    "function poincaré(μ, G, x0; solver_args...)\n",
    "  sol = solve_neurons(μ, G, jump_map(μ, x0);\n",
    "                      termination_indices=(1,),\n",
    "                      save_everystep=false, solver_args...)\n",
    "  sol.t[end], sol[end]\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    period_and_fp(μ, G)\n",
    "\n",
    "For a parameter set μ and a connectivity matrix G, find the fixed\n",
    "point of the Poincaré map and return the corresponding first return\n",
    "time. This is done through simple Picard fixed-point iteration.\n",
    "\"\"\"\n",
    "function period_and_fp(μ, G, x0=nothing; maxiter=500, warn=true,\n",
    "                       atol=1e-1, rtol=1e-3, pc_args...)\n",
    "\n",
    "  if x0 === nothing\n",
    "    x0 = x0_fire1(μ)\n",
    "  end\n",
    "\n",
    "  for iter in 1:maxiter\n",
    "    xprev = x0\n",
    "    T,x0 = poincaré(μ, G, x0; pc_args...)\n",
    "\n",
    "    if isapprox(x0, xprev; atol=atol, rtol=rtol)\n",
    "      return T==Inf ? 0 : T, x0\n",
    "    end\n",
    "  end\n",
    "\n",
    "  if warn\n",
    "    @warn \"Convergence failed after $(maxiter) iterations.\"\n",
    "  end\n",
    "  return Inf, x0\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    modify_type(type; kw...)\n",
    "\n",
    "Take a neuron type and return a new type where all the keywords in kw\n",
    "have been applied to replace the corresponding parameters.\n",
    "\"\"\"\n",
    "modify_type(type; kw...) = merge(type, kw)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_mod1(;kw...)\n",
    "\n",
    "Modifies the RS type of  the first neuron in a module.\n",
    "\"\"\"\n",
    "μ_mod1(; kw...) = μ_from_types(modify_type(RS; kw...), RS, LTS)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_modsame(;kw...)\n",
    "\n",
    "Modifies the RS type of  the first neuron in a module.\n",
    "\"\"\"\n",
    "μ_modsame(; kw...) = let type = modify_type(RS; kw...)\n",
    "  μ_from_types(type, type, LTS)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    randomize_type(type, frac; kw...)\n",
    "\n",
    "Take a neuron type and an amount of fractional variation, then apply\n",
    "that amount of variation to each of the parameters of the type. If a\n",
    "keyword argument is given, this is used as an absolute radius of\n",
    "variation instead, in order to handle parameters equal to zero. Return\n",
    "the applied fractional variation ξ as well as the actual parameters as\n",
    "two separate dictionaries.\n",
    "\"\"\"\n",
    "function randomize_type(type, frac; kw...)\n",
    "  ξ = Dict(zip(keys(type), randsphere(length(type), frac)))\n",
    "  Dict(k => v + get(kw, k, v)*ξ[k] for (k,v) in type), ξ\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    module_works(μ, G; [Tmin], [x0])\n",
    "\n",
    "Quantify whether a module works: the default initial condition must be\n",
    "within the basin of attraction of a spiking limit cycle with period at\n",
    "least equal to the given minimum period.\n",
    "\"\"\"\n",
    "function module_works(μ, G; Tmin=0.0, x0=nothing, fp_args...)\n",
    "  Tmin < period_and_fp(μ, G, x0; fp_args...)[1] < Inf\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    randsphere(d, r=1)\n",
    "\n",
    "Generate points uniformly at random on the radius-r (d-1)-sphere in ℝᵈ.\n",
    "\"\"\"\n",
    "randsphere(d, r=1) = let gaussblob = randn(d)\n",
    "  r .* gaussblob ./ norm(gaussblob)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    randball(d, r=1)\n",
    "\n",
    "Generate points uniformly at random in the d-ball of radius r.\n",
    "\"\"\"\n",
    "randball(d, r=1) = randsphere(d,r) .* (rand() ^ (1/d))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_rand1(frac)\n",
    "\n",
    "Generate a parameter array corresponding to random variation in the\n",
    "parameters of the first neuron, returning also the fractional change\n",
    "ξ. Variation in the excitatory synaptic reversal potential is absolute\n",
    "and multiplied by 100 mV because its default value is exactly zero.\n",
    "\"\"\"\n",
    "function μ_rand1(frac)\n",
    "  type, ξ = randomize_type(RS, frac; Vn=100)\n",
    "  μ_from_types(type, RS, LTS), ξ\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_randsame(frac)\n",
    "\n",
    "Generate a parameter array corresponding to random variation in the\n",
    "parameters of the excitatory neurons, with both cells modulated in\n",
    "exactly the same way. Also returns the modification applied.\n",
    "\"\"\"\n",
    "function μ_randsame(frac)\n",
    "  type, ξ = randomize_type(RS, frac; Vn=100)\n",
    "  μ_from_types(type, type, LTS), ξ\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    μ_rand2(frac)\n",
    "\n",
    "Generate a parameter array corresponding to random variation in the\n",
    "parameters of the excitatory neurons, returning also the fractional\n",
    "changes ξ1 and ξ2.\n",
    "\"\"\"\n",
    "function μ_rand2(frac)\n",
    "  type1, ξ1 = randomize_type(RS, frac; Vn=100)\n",
    "  type2, ξ2 = randomize_type(RS, frac; Vn=100)\n",
    "  μ_from_types(type1, type2, LTS), ξ1, ξ2\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Figure 2: Module Electrical Activity Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This figure introduces a bit of extra synaptic input to the inhibitory interneuron within the module in order to cause it to deactivate the module after a short period of activity. The synaptic matrix is the same as the default connectivity matrix for the module, but with a stronger connection from the excitatory cells of the module to the reset interneuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "μ, G = μ_module, Gij(; G_rst=7.0747)\n",
    "\n",
    "sol = solve_neurons(μ, G, jump_map(μ, x0_fire1(μ)); tmax=1000)\n",
    "\n",
    "fig, axes = Plt.subplots(2,1)\n",
    "\n",
    "t = range(0,150; length=1000)\n",
    "axes[1].plot(t, [u.x[1] for u in sol(t)])\n",
    "axes[2].plot(t, [G*(u.x[3] .* μ.x[9]) .- (G*u.x[3]) .* u.x[1]\n",
    "                    for u in sol(t)])\n",
    "\n",
    "for ax in axes\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for side in [:top, :bottom, :left, :right]\n",
    "        ax.spines[string(side)].set_visible(false)\n",
    "    end\n",
    "end\n",
    "\n",
    "axes[1].set_ylabel(\"Membrane Voltage\")\n",
    "axes[end].set_ylabel(\"Synaptic Input\")\n",
    "fig.legend((\"E1\", \"E2\", \"I\"); ncol=3, loc=\"lower center\")\n",
    "\n",
    "# fig.savefig(\"Fig2.tif\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "source": [
    "![alt text](figures/Fig2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Figure 3: Effect of Parameter Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "fig, axs = Plt.subplots(4,1)\n",
    "\n",
    "t = range(1,100; length=1000)\n",
    "\n",
    "for ax in axs\n",
    "    μ = if ax === axs[1] \n",
    "        μ_module\n",
    "    else\n",
    "        μ_rand2(0.1)[1]\n",
    "    end\n",
    "    \n",
    "    sol = solve_neurons(μ, Gij(), jump_map(μ, x0_fire1(μ)); tmax=2e2)\n",
    "    ax.plot(t, [u.x[1][1:2] for u in sol(t.+100)])\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0,100)\n",
    "    \n",
    "    for side in [\"top\", \"bottom\", \"left\", \"right\"]\n",
    "        ax.spines[side].set_visible(false)\n",
    "    end\n",
    "end\n",
    "\n",
    "fig.legend(axs[end].lines, [raw\"$E_1$\", raw\"$E_2$\"]; \n",
    "    ncol=3, loc=\"lower right\")\n",
    "axs[end].set_xticks([])\n",
    "axs[end].set_xlabel(\"\")\n",
    "\n",
    "fig.text(0.05, 0.5, \"Membrane Voltage\"; rotation=\"vertical\", va=\"center\", size=8)\n",
    "# fig.savefig(\"Fig3.tif\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "source": [
    "![alt text](figures/Fig3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Method of Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We determine the exact point where a parameter leaves its acceptable range using the method of bisection. This is implemented in the following relatively general-purpose code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\"\"\"\n",
    "    bisect(P, a, b; atol=0, rtol=√ε)\n",
    "\n",
    "Uses the bisection method to narrow an interval where some predicate\n",
    "changes sign until its endpoints are equal with relative precision\n",
    "`rtol` and absolute precision `atol`. Alternately, if the optional\n",
    "argument `sigfigs` is provided, the tolerance is set to produce a\n",
    "result accurate to that many significant figures.\n",
    "\"\"\"\n",
    "function bisect(P, a, b; sigfigs=nothing, kw...)\n",
    "  Pa, Pb = P(a), P(b)\n",
    "  @assert(Pa != Pb, \"Truth value of predicate must differ at bounds.\")\n",
    "\n",
    "  if sigfigs !== nothing\n",
    "    kw = (rtol=10.0^-sigfigs,\n",
    "          atol=abs(a-b)*10.0^-sigfigs)\n",
    "  end\n",
    "\n",
    "  while !isapprox(a,b; kw...)\n",
    "    guess = (a+b)/2\n",
    "\n",
    "    if P(guess) == Pa\n",
    "      a = guess\n",
    "    else\n",
    "      b = guess\n",
    "    end\n",
    "  end\n",
    "\n",
    "  a, b\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    refine_bound(P, x0, xguess; rtol, atol)\n",
    "\n",
    "For a predicate true at x0 but false at xguess, refine the bound by\n",
    "bisection.\n",
    "\"\"\"\n",
    "function refine_bound(P, x0, xguess; sigfigs=nothing, kw...)\n",
    "  x = bisect(P, x0, xguess; sigfigs, kw...)[1]\n",
    "\n",
    "  if sigfigs === nothing\n",
    "    x\n",
    "  else\n",
    "    round(x; sigdigits=sigfigs)\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    refine_bounds(P, x0, xlow, xhigh; rtol, atol)\n",
    "\n",
    "For a predicate true at x0 but false and xlow and xhigh,\n",
    "simultaneously refine both bounds by bisection.\n",
    "\"\"\"\n",
    "function refine_bounds(P, x0, xlow, xhigh; kw...)\n",
    "  (refine_bound(P, x0, xlow; kw...),\n",
    "   refine_bound(P, x0, xhigh; kw...))\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Acceptable Parameter Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can compute the acceptable range of variation in each single-neuron parameter by checking whether the spiking limit cycle continues to exist as described in the text. These all follow the same pattern of ensuring that the standard initial condition leads to an attractive fixed point of the Poincaré map. We can also do exactly the same thing, mathematically speaking, with the single-module synaptic conductance parameters $G_\\text{exc}$ and $G_\\text{rst}$: they both cease to function when the spiking limit cycle disappears.\n",
    "\n",
    "On the other hand, as described in the text, the simplest way to check that a value of $G_\\text{inh}$ is acceptable is to increase $G_\\text{rst}$ enough that persistent activity causes the reset interneuron to fire; if this breaks the limit cycle and returns the neuron to a resting state, the reset strength is sufficient. As a result, we check that perturbed values of $G_\\text{inh}$ function correctly by checking that they are not acceptable in conjunction with a perturbed value of $G_\\text{rst}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "a_bound = refine_bound(RS[:a], 0.001; sigfigs=3) do a\n",
    "    module_works(μ_modsame(a=a), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "b_bound = refine_bound(RS[:b], 10*RS[:b]; sigfigs=3) do b\n",
    "    module_works(μ_modsame(b=b), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "c_bound = refine_bound(RS[:c], 0; sigfigs=3) do c\n",
    "    module_works(μ_modsame(c=c), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "d_bounds = refine_bounds(RS[:d], -RS[:d], 10*RS[:d]; sigfigs=3) do d\n",
    "    module_works(μ_modsame(d=d), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "C_bounds = refine_bounds(RS[:C], 0.1*RS[:C], 10*RS[:C]; sigfigs=3) do C\n",
    "    module_works(μ_modsame(C=C), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "k_bound = refine_bound(RS[:k], 10*RS[:k]; sigfigs=3) do k\n",
    "    module_works(μ_modsame(k=k), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Vr_bounds = refine_bounds(RS[:Vr], 10*RS[:Vr], -20.0; sigfigs=3) do Vr\n",
    "    module_works(μ_modsame(Vr=Vr), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Vt_bounds = refine_bounds(RS[:Vt], 10*RS[:Vt], -20.0; sigfigs=3) do Vt\n",
    "    module_works(μ_modsame(Vt=Vt), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Vp_bound = refine_bound(RS[:Vp], RS[:Vt]; sigfigs=3) do Vp\n",
    "    module_works(μ_modsame(Vp=Vp), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Vn_bounds = refine_bounds(RS[:Vn], RS[:Vt], 10.0; sigfigs=3) do Vn\n",
    "    module_works(μ_modsame(Vn=Vn), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "τ_bounds = refine_bounds(RS[:τ], 1.0, 10.0; sigfigs=3) do τ\n",
    "    module_works(μ_modsame(τ=τ), Gij(); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Gexc_bounds = refine_bounds(20.0, 15.0, 200.0; sigfigs=3) do G_exc\n",
    "    module_works(μ_module, Gij(; G_exc); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Grst_bound = refine_bound(5.0, 10.0; sigfigs=3) do G_rst\n",
    "    module_works(μ_module, Gij(; G_rst); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "Ginh_bound = refine_bound(10.0, 0.0; sigfigs=3) do G_inh\n",
    "    !module_works(μ_module, Gij(; G_inh, G_rst=10.0); Tmin=5.0, warn=false)\n",
    "end\n",
    "\n",
    "println(\"Changing both cells equally, the module still works for...\")\n",
    "println(\"    a > $a_bound\")\n",
    "println(\"    b > $b_bound\")\n",
    "println(\"    c < $c_bound\")\n",
    "println(\"    d ∈ $d_bounds\")\n",
    "println(\"    C ∈ $C_bounds\")\n",
    "println(\"    k < $k_bound\")\n",
    "println(\"   Vᵣ ∈ $Vr_bounds\")\n",
    "println(\"   Vₜ ∈ $Vt_bounds\")\n",
    "println(\"   Vₚ > $Vp_bound\")\n",
    "println(\"   Vₙ ∈ $Vn_bounds\")\n",
    "println(\"    τ ∈ $τ_bounds\")\n",
    "println(\" Gexc ∈ $Gexc_bounds\")\n",
    "println(\" Grst < $Grst_bound\")\n",
    "println(\" Ginh > $Ginh_bound\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "Changing both cells equally, the module still works for...\n",
    "    a > 0.0171\n",
    "    b > -6.08\n",
    "    c < -40.3\n",
    "    d ∈ (40.4, 169.0)\n",
    "    C ∈ (68.3, 159.0)\n",
    "    k < 1.41\n",
    "   Vᵣ ∈ (-66.3, -29.2)\n",
    "   Vₜ ∈ (-47.0, -36.2)\n",
    "   Vₚ > -32.7\n",
    "   Vₙ ∈ (-9.69, 9.67)\n",
    "    τ ∈ (3.77, 7.41)\n",
    " Gexc ∈ (16.1, 31.6)\n",
    " Grst < 7.07\n",
    " Ginh > 4.37\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we set up and execute Monte Carlo simulations to verify that the majority of CPG modifications are harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\"The number of CPG realizations to test for computing statistics.\"\n",
    "N_montecarlo = 1000000\n",
    "\n",
    "\"\"\"\n",
    "    try_many(sample, N)\n",
    "\n",
    "Try N different random variants of the module, recording both the\n",
    "random modification to the parameters and whether the module still\n",
    "works under those conditions.\n",
    "\"\"\"\n",
    "function try_many(sample, N)\n",
    "\n",
    "  ξs = zeros(N, length(sample()[2]))\n",
    "  worksp = falses(N)\n",
    "\n",
    "  Threads.@threads for i in 1:N\n",
    "    μ, ξs[i,:] = sample()\n",
    "    worksp[i] = module_works(μ, Gij(); warn=false)\n",
    "  end\n",
    "  ξs, worksp\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "@time ξs1, worksp1 = try_many(N_montecarlo) do\n",
    "    μ, ξ = μ_rand1(0.10)\n",
    "    μ, [ξ[p] for p in parameter_names]\n",
    "end\n",
    "\n",
    "println(\"Varying one neuron, modules break \", \n",
    "    round(100*(1 - mean(worksp1)); digits=1), \"% of the time.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "848.728239 seconds (15.74 G allocations: 1.465 TiB, 63.61% gc time)\n",
    "Varying one neuron, modules break 1.3% of the time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "@time ξssame, workspsame = try_many(N_montecarlo) do\n",
    "    μ, ξ = μ_randsame(0.10)\n",
    "    μ, [ξ[p] for p in parameter_names]\n",
    "end\n",
    "\n",
    "println(\"Varying both neurons identically, modules break \", \n",
    "    round(100*(1 - mean(workspsame)); digits=1), \"% of the time.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "875.569545 seconds (15.55 G allocations: 1.447 TiB, 63.08% gc time)\n",
    "Varying both neurons identically, modules break 2.8% of the time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "@time ξs2, worksp2 = try_many(N_montecarlo) do\n",
    "    μ, ξ1, ξ2 = μ_rand2(0.10)\n",
    "    μ, hcat([ξ1[p] for p in parameter_names],\n",
    "            [ξ2[p] for p in parameter_names])\n",
    "end\n",
    "\n",
    "println(\"Varying both neurons independently, modules break \", \n",
    "    round(100*(1 - mean(worksp2)); digits=1), \"% of the time.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "909.190263 seconds (16.10 G allocations: 1.501 TiB, 63.31% gc time)\n",
    "Varying both neurons independently, modules break 5.4% of the time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Figure 4: LDA Point Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here, we identify the linear discriminant axis as well as a semi-arbitrary orthogonal axis of a smaller version of the point cloud corresponding to simultaneous variation of both neurons' parameter vectors. The vector is printed out in order to show the most important components ($V_t$, $V_r$, and $V_n$), and then the point cloud projected onto those two axes is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "# Use LDA to find the most explanatory axis of the results from the Monte Carlo \n",
    "# simulation of random variation in both excitatory neurons' parameters.\n",
    "Xp = permutedims(ξssame[workspsame, :])\n",
    "Xn = permutedims(ξssame[ .! workspsame, :])\n",
    "lda = fit(LinearDiscriminant, Xp, Xn)\n",
    "firstaxis = normalize(lda.w)\n",
    "\n",
    "# Project that away and find a second, then orthogonalize.\n",
    "secondguess = try\n",
    "    Xp2 = Xp .- (firstaxis' * Xp) .* firstaxis\n",
    "    Xn2 = Xn .- (firstaxis' * Xn) .* firstaxis\n",
    "    fit(LinearDiscriminant, Xp2, Xn2).w\n",
    "catch\n",
    "    randn(length(firstaxis))\n",
    "end\n",
    "secondaxis = normalize(secondguess .- firstaxis .* (firstaxis ⋅ secondguess))\n",
    "\n",
    "# Print out the first axis to identify its main components. You can see that the \n",
    "# most important components (magnitude about 0.5) correspond to Vt, Vr, and Vn.\n",
    "round.(firstaxis * 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "11-element Array{Float64,1}:\n",
    "  1.0\n",
    "  0.0\n",
    " -0.0\n",
    " -1.0\n",
    " -1.0\n",
    " -1.0\n",
    " -5.0\n",
    "  6.0\n",
    "  6.0\n",
    "  0.0\n",
    "  2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "# Choose 1000 new points to plot, with higher variance for illustrative purposes.\n",
    "ξsub, workspsub = try_many(1000) do\n",
    "    μ, ξ1 = μ_randsame(0.15)\n",
    "    μ, [ξ1[p] for p in parameter_names]\n",
    "end\n",
    "\n",
    "# Construct a projection matrix and apply it to the new points.\n",
    "projection = [firstaxis secondaxis]'\n",
    "Xpproj = projection * permutedims(ξsub[workspsub, :])\n",
    "Xnproj = projection * permutedims(ξsub[ .!workspsub, :])\n",
    "\n",
    "# This part actually plots the point cloud.\n",
    "fig = Plt.figure()\n",
    "ax = fig.gca(aspect=:equal)\n",
    "ax.plot(Xpproj[1,:], Xpproj[2,:], \".\", ms=3)\n",
    "ax.plot(Xnproj[1,:], Xnproj[2,:], \".\", ms=3)\n",
    "\n",
    "# And all the rest is just making it look prettier for the paper. :)\n",
    "fig.legend([\"Working\", \"Non-working\"]; \n",
    "    loc=\"upper right\", markerscale=2,\n",
    "    bbox_to_anchor=(0.9, 0.95))\n",
    "ax.spines[\"right\"].set_visible(false)\n",
    "ax.spines[\"top\"].set_visible(false)\n",
    "ax.set_xticks([-0.1, 0.1])\n",
    "ax.set_xticklabels([raw\"-10\\%\", raw\"+10\\%\"])\n",
    "ax.set_xlabel(\"Primary Discriminant Axis\")\n",
    "ax.set_yticks([-0.1, 0.1])\n",
    "ax.set_yticklabels([raw\"-10\\%\", raw\"+10\\%\"])\n",
    "ax.set_ylabel(\"Secondary Discriminant Axis\")\n",
    "# fig.savefig(\"Fig4.tif\"; bbox_inches=:tight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "source": [
    "![alt text](figures/Fig4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simulating a Custom CPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code implements the neural network corresponding to the full four-module CPG developed in section 4 of the paper. This CPG consists of twelve neurons making up four identical modules, plus four muscle cells used to control the actuators. (It would be eight, except that the inputs to a given actuator's flexor and the extensor of its complementary actuator are identical, leading to identical behavior, so they are not simulated separately. This is a meaningless and potentially premature optimization.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "\"\"\"\n",
    "    Gcpg(; G_fb, G_ffw, G_exc, G_inh, G_rst)\n",
    "\n",
    "Generate a synaptic connectivity matrix for the full four-module CPG.\n",
    "There are also four muscle cells, which receive a weak connection of\n",
    "strength G_musc. The idea is that this should be sufficient to create\n",
    "a deviation in membrane voltage of about 1 mV.\n",
    "\"\"\"\n",
    "function Gcpg(; G_fb=G_fb, G_ffw=G_ffw, G_musc=1.0, Gij_args...)\n",
    "  # Four three-cell modules plus four muscles.\n",
    "  G = zeros(16,16)\n",
    "  # Generate four modules this way.\n",
    "  G[1:12,1:12] .= kron(Matrix(1.0I, 4, 4), Gij(Gij_args...))\n",
    "  G[1,11] = G[4,2] = G[7,5] = G[10,8] = G_fb\n",
    "  G[3,4] = G[6,7] = G[9,10] = G[12,1] = G_ffw\n",
    "  # Now add the forward connections to the muscles.\n",
    "  G[13,2] = G[14,5] = G[15,8] = G[16,11] = G_musc\n",
    "  # And return the thing!\n",
    "  G\n",
    "end\n",
    "\n",
    "\"Parameter array for the full CPG, including muscles.\"\n",
    "μcpg = μ_from_types(RS, RS, LTS, RS, RS, LTS, RS, RS, LTS, RS, RS, LTS,\n",
    "                    RS, RS, RS, RS)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    solve_cpg(μ, G)\n",
    "\n",
    "Inelegant copypasta of solve_neurons, plus some glue code to make it\n",
    "simulate the simple mass-damper system which stands in for the robot.\n",
    "\"\"\"\n",
    "function solve_cpg(μ, G, p; tmax=Inf, fp_tol=1e-4, solver_args...)\n",
    "\n",
    "  # The in-place continuous dynamics of the neurons and actuators.\n",
    "  function izh(dx, x, (μ,G,p), t)\n",
    "    a,b,c,d, C,k,vr,vt,vn,vp, τ = μ.x\n",
    "\n",
    "    sodium = @.k*(x.x[1] - vr)*(x.x[1] - vt)\n",
    "    Isyn = G*(x.x[3] .* vn)  .-  (G*x.x[3]) .* x.x[1]\n",
    "    dx.x[1] .= @. (sodium - x.x[2] + Isyn)/C\n",
    "    dx.x[2] .= @. a*(b*(x.x[1]-vr) - x.x[2])\n",
    "    dx.x[3] .= @. x.x[4] / τ\n",
    "    dx.x[4] .= @. -(x.x[3] + 2x.x[4]) / τ\n",
    "\n",
    "    # Fortunately, actuator dynamics are super simple, except for the\n",
    "    # mess of ensuring that the actuators stay in bounds. Just make\n",
    "    # sure that the velocity and position don't increase when the\n",
    "    # position is already at the upper bound, and don't decrease when\n",
    "    # it's at the lower bound.\n",
    "    activations = x.x[1][end-3:end] - circshift(x.x[1][end-3:end], 2)\n",
    "    activations = clamp.(activations, -1.0, 1.0)\n",
    "    dx.x[5] .= x.x[6]\n",
    "    dx.x[6] .= @. (activations - p[2]*x.x[6]) / p[1]\n",
    "    dx.x[5][@. (x.x[5] >= 1.0) & (x.x[6] >= 0.0)] .= 0.0\n",
    "    dx.x[6][@. (x.x[5] >= 1.0) & (dx.x[6] >= 0.0)] .= 0.0\n",
    "    dx.x[5][@. (x.x[5] <= 0.0) & (x.x[6] <= 0.0)] .= 0.0\n",
    "    dx.x[6][@. (x.x[5] <= 0.0) & (dx.x[6] <= 0.0)] .= 0.0\n",
    "\n",
    "    # Finally, here's the proprioceptive feedback.\n",
    "    Ifb = -p[3] .* (1 .- circshift(x.x[5], 1) .+ circshift(x.x[5], -1))\n",
    "    for i in 1:4\n",
    "      dx.x[1][3i-2] += Ifb[i] / C[3i-2]\n",
    "      dx.x[1][3i-1] += Ifb[i] / C[3i-1]\n",
    "    end\n",
    "  end\n",
    "\n",
    "\n",
    "  # This is the callback that produces the spiking behavior. It is\n",
    "  # vectorized and done in-place for efficiency. The first function\n",
    "  # condition() checks whether any voltage exceeds the corresponding\n",
    "  # cell's Vp, and the second function affect!() does the reset for\n",
    "  # that cell (or terminates if it's on the termination list).\n",
    "  function should_fire(out, u,t,integrator)\n",
    "    out .= u.x[1] .- integrator.p[1].x[10]\n",
    "  end\n",
    "\n",
    "  function spike_reset!(integrator, i)\n",
    "    jump_neuron!(i, integrator.u, integrator.p[1])\n",
    "  end\n",
    "\n",
    "  # The VectorContinuousCallback needs to know how many callbacks\n",
    "  # there are, which is the number of neurons. Get that information\n",
    "  # from the number of values given for an arbitrary parameter.\n",
    "  spike = DE.VectorContinuousCallback(should_fire, spike_reset!,\n",
    "                                      length(μ.x[1]))\n",
    "\n",
    "  # Also add callbacks for the actuator bounds.\n",
    "  function actuator_bounds(out, u,t,integrator)\n",
    "    out[1:4] .= -u.x[5]\n",
    "    out[5:8] .= u.x[5] .- 1.0\n",
    "  end\n",
    "\n",
    "  function actuator_reset!(integrator, i)\n",
    "    if i > 4\n",
    "      integrator.u.x[5][i-4] = 1.0\n",
    "      integrator.u.x[6][i-4] = 0.0\n",
    "    else\n",
    "      integrator.u.x[5][i] = 0.0\n",
    "      integrator.u.x[6][i] = 0.0\n",
    "    end\n",
    "  end\n",
    "\n",
    "  # Only count this callback in the positive direction so we can get\n",
    "  # away from the walls without causing an explosion.\n",
    "  crash = DE.VectorContinuousCallback(actuator_bounds, actuator_reset!,\n",
    "                                      nothing, 8)\n",
    "\n",
    "  # An initial condition including the states of all the neurons in\n",
    "  # the CPG as well as the four actuators' position and velocity.\n",
    "  x0 = ArrayPartition(jump_map(μcpg, x0_fire1(μcpg)).x...,\n",
    "                      0.5ones(4), zeros(4))\n",
    "\n",
    "  # Solve the problem and return it!\n",
    "  prob = DE.ODEProblem(izh, x0, (0.0, tmax), (μ,G,p))\n",
    "  DE.solve(prob, DE.Tsit5(), callback=DE.CallbackSet(spike, crash);\n",
    "           solver_args...)\n",
    "end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Figure 7: Effect of Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here, we compare two copies of the constructed CPG, with and without proprioceptive feedback, in order to demonstrate the inability of the open-loop system to locomote effectively even though it is a correct implementation of the desired state machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "@time ol = solve_cpg(μcpg, Gcpg(), (20e3,570.0,0.0); tmax=10000.0);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "10.774645 seconds (40.95 M allocations: 2.329 GiB, 5.15% gc time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "@time cl = solve_cpg(μcpg, Gcpg(), (20e3,570.0,25.0); tmax=10000.0);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2.185406 seconds (11.43 M allocations: 862.187 MiB, 5.88% gc time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` julia\n",
    "fig, axes = Plt.subplots(10,1; figsize=(3,4.2))\n",
    "\n",
    "for (i,ax) in enumerate(axes[1:4])\n",
    "    ax.axhline(0; c=\"grey\", lw=0.5)\n",
    "    ax.plot(ol.t, [u.x[1][3i-2] for u in ol.u]; c=:C0)\n",
    "    ax.plot(ol.t, [u.x[1][3i-1] for u in ol.u]; c=:C0)\n",
    "    ax.plot(cl.t, [u.x[1][3i-2] for u in cl.u]; c=:C1)\n",
    "    ax.plot(cl.t, [u.x[1][3i-1] for u in cl.u]; c=:C1)\n",
    "end\n",
    "\n",
    "# Plot actuator positions with scale lines, and save two lines to label in the legend.\n",
    "line_ol = line_cl = nothing\n",
    "for (i,ax) in enumerate(axes[6:9])\n",
    "    ax.axhline(0.5; c=\"grey\", lw=0.5)\n",
    "    line_ol = ax.plot(ol.t, [u.x[5][i] for u in ol.u]; c=:C0)[1]\n",
    "    line_cl = ax.plot(cl.t, [u.x[5][i] for u in cl.u]; c=:C1)[1]\n",
    "    ax.set_ylim(0,1)\n",
    "end\n",
    "\n",
    "# Clean up all those annoying borders.\n",
    "for ax in axes\n",
    "    ax.set_xlim(3000, 8000)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for side in (\"left\", \"right\", \"top\", \"bottom\")\n",
    "        ax.spines[side].set_visible(false)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Freaky axis labels because there's no room to label them separately.\n",
    "fig.text(0.05, 0.35, \"Actuator Position\", rotation=:vertical, va=:center, size=8)\n",
    "fig.text(0.05, 0.73, \"Membrane Voltage\", rotation=:vertical, va=:center, size=8)\n",
    "\n",
    "# Title and save it!\n",
    "fig.legend([line_ol, line_cl], [\"Open-Loop\", \"Closed-Loop\"]; \n",
    "    loc=\"center right\", bbox_to_anchor=(0.8, 0.45))\n",
    "# fig.savefig(\"Fig7.tif\"; bbox_inches=:tight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "source": [
    "![alt text](figures/Fig7.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
